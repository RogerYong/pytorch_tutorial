{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NEURAL NETWORKS](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "\n",
    "torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import numpy\n",
    "import time\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Define a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和autograd一样,nn也是torch的一个包.\n",
    "\n",
    "nn依赖于autograd的梯度/网络的定义。\n",
    "\n",
    "nn.Module 有layers的概念。\n",
    "\n",
    "\n",
    "feed-forward(前馈)\n",
    "\n",
    "在输入层,使用forward(input), 返回output结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/mnist.png)\n",
    "\n",
    "这里实现一个LeNet\n",
    "\n",
    "注意: 图片是5x5的LeNet\n",
    "下面代码是3x3的Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5x5 LeNet-5\n",
    "\n",
    "输入 32x32\n",
    "\n",
    "conv1: 5x5,stride=1, 输出 $\\frac{32-5}{stride}+1 = 28$\n",
    "\n",
    "max_pool1 : 2x2,stride 2，输出 $\\frac{28-2}{stride}+1 = 14$\n",
    "\n",
    "\n",
    "conv2: 5x5,stride 1 ,输出 $\\frac{14-5}{stride}+1 = 10$\n",
    "\n",
    "max_pool2 : 2x2,stride 2，输出 $\\frac{10-2}{stride}+1 = 5$\n",
    "\n",
    "这时图片是 5*5的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3x3 LeNet:\n",
    "\n",
    "输入 32x32\n",
    "\n",
    "conv1:6个 3x3 x1 ,stride=1, 输出 $\\frac{32-3}{stride}+1 = 30$\n",
    "\n",
    "max_pool1 : 2x2,stride 2，输出 $\\frac{30-2}{stride}+1 = 15$\n",
    "\n",
    "\n",
    "conv2:16个 3x3 x6,stride 1 ,输出 $\\frac{15-3}{stride}+1 = 13$\n",
    "\n",
    "max_pool2 : 2x2,stride 2，输出 $\\frac{13-2}{stride}+1 = 6$\n",
    "\n",
    "这时图片是 6*6 的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Q: conv是卷积层,输入层在哪里?\n",
    "        # A: 这里定义了模型的参数。\n",
    "        #    其他的非参数层（输入层, 激活函数层，池化层等）在forward中定义\n",
    "        # nn.Module用于参数的封装\n",
    "        \n",
    "        # 当然 max_pool relu也可以在这里定义 \n",
    "        # 例子\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.relu = nn.ReLU()\n",
    "\n",
    "        # 注意：\n",
    "        # LeNet-5原来是5x5的卷积核，fc1输入大小是 16 * 5 * 5\n",
    "        # 这里是3x3, 因此fc1输入大小是 16 * 6 * 6\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  # 3x3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)  # 3x3\n",
    "\n",
    "        # 卷积的输入可以是任意的\n",
    "        # 但是，全连接层的输入必须指定大小\n",
    "        # 因此，整个网络的输入大小是固定的\n",
    "\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # functional与nn\n",
    "        # 至于卷积层，输入层，输出层，全连接层,是实际保存数据的“层”,是 tensor Variable\n",
    "        # 其中,卷积层，全连接层保存的是参数， 是神经网络 (nn)， 要计算梯度,要激活\n",
    "\n",
    "        # 激活函数,池化层, 不是“层”,而是和 + - * / 类似的一个函数, 因此是functional\n",
    "\n",
    "        # nn.conv2d 和 F.max_pool2d 的尺寸指定, 可以是 (h,w) ,也可以直接一个数值\n",
    "        # max pool 1 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # max pool 2 2x2\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "\n",
    "        # 二维矩阵 转 向量\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "\n",
    "        # 另一种写法\n",
    "        # hard ocde LeNet 3*3\n",
    "        # x = x.view(-1,36)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # 最后一个不用激活？计算10个分类的可能性\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 第0维是batch size，后面是特征维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结\n",
    "\n",
    "* nn.Module 用于参数的封装，形成一个网络Module\n",
    "* nn.Paramter 一种Tensor, 作为Module的属性。例如, Conv Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing inputs and calling backward\n",
    "autograd auto在哪里?\n",
    "\n",
    "我们只需要定义forward\n",
    "autograd自动backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数的定义清晰明了 & 参数的计算\n",
    "卷积层 全连接层 的参数分两部分 W 和 b\n",
    "\n",
    "卷积参数\n",
    "W 卷积个数 * 输入深度 * 高 * 宽\n",
    "b 卷积个数 \n",
    "\n",
    "全连接层\n",
    "W 输入的向量维度\n",
    "b 输出的向量维度\n",
    "\n",
    "\n",
    "上面的LeNet有2个卷积层，3个全连接层\n",
    "共10个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "for  i in range(len(params)):\n",
    "    print(params[i].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0570, -0.1049,  0.0301,  0.0120,  0.0016,  0.0582, -0.1153,  0.0798,\n",
      "          0.1301, -0.1239]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)  # 自动调用forward, batch, size 1 \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn 只支持mini-batches,不支持单样本\n",
    "\n",
    "单样本处理方法：\n",
    "* 可以处理成size=1的batch\n",
    "* input.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() # 把参数的梯度清零, 避免错误\n",
    "out.backward(torch.randn(1,10)) \n",
    "\n",
    "# 如何打印梯度呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6746, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10) # label\n",
    "\n",
    "target = target.view(1,-1) # batch size 1 \n",
    "\n",
    "criterion = nn.MSELoss() # mean square error 均方误差\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过 grad_fn 和 next_functions 来保存记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fe1991a8c18>\n",
      "<AddmmBackward object at 0x7fe1991a8c50>\n",
      "<AccumulateGrad object at 0x7fe1991a8c18>\n",
      "((<AddmmBackward object at 0x7fe1991a8cc0>, 0),)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear [0][1] 是什么意思\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
    "\n",
    "print(loss.grad_fn.next_functions) \n",
    "print(len(loss.grad_fn.next_functions)) # 可能有很多个分支"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x7fe1991a8e80>\n",
      "((<AccumulateGrad object at 0x7fe1991a8dd8>, 0), (<AddBackward0 object at 0x7fe1991a8e48>, 0))\n",
      "((<AccumulateGrad object at 0x7fe1991a8e80>, 0), (None, 0))\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1, requires_grad=True)\n",
    "b = a*(a+2)\n",
    "print (b.grad_fn)\n",
    "print (b.grad_fn.next_functions)\n",
    "print (b.grad_fn.next_functions[1][0].next_functions)\n",
    "print (b.grad_fn.next_functions[0][0].variable is a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0122,  0.0092, -0.0261, -0.0123,  0.0125, -0.0110])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降 更新权值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 内置优化方法\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "```\n",
    "weight = weight - learning_rate * gradient\n",
    "```\n",
    "\n",
    "\n",
    "SGD\n",
    "Adam 动量方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(),lr=0.01)               \n",
    "for i in range(1):  # 循环输入数据\n",
    "    optimizer.zero_grad()\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
