{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "[AUTOGRAD: AUTOMATIC DIFFERENTIATION](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor\n",
    "\n",
    "* use grad\n",
    "\n",
    "set tensors' attribute `.requires_grad` as `True`\n",
    "\n",
    "call `.backward()`\n",
    "\n",
    "\n",
    "The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "* no grad\n",
    "\n",
    "prevent tracking history, save memory\n",
    "\n",
    "wrap the code block in` with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requires_grad 与 grad_fn属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建时指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数计算结果 与 grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f70939e3898>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改requires_grad属性的方法\n",
    "\n",
    "* a.requires_grad_(True) 函数\n",
    "* 直接修改属性 a.requires_grad = True\n",
    "* with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7f70939f37f0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()  # 只能在标量上运行backward\n",
    "\n",
    "# out.backward(retain_graph=True) # 保留buffer，可以重新推导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-1c48c9d7e862>\", line 3, in <module>\n",
      "    out.backward()  # backward一次,buffer会被清空\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/tensor.py\", line 107, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 93, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "try:\n",
    "    out.backward()  # backward一次,buffer会被清空\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "print(y)\n",
    "print(y.grad)  # 没有required,不记录梯度, backward一次,buffer会被清空\n",
    "print(z.grad)  # 没有required,不记录梯度, backward一次,buffer会被清空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd\n",
    "\n",
    "运行在张量上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([0., 0.])\n",
      "tensor([13., 13.], grad_fn=<CopySlices>)\n",
      "tensor([6., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2, 3], dtype=torch.float, requires_grad=True)\n",
    "print(x)\n",
    "y = torch.zeros(2)\n",
    "print(y)\n",
    "y[0] = x[0]**2 + x[1] * 3\n",
    "y[1] = x[0] * 2 + x[1]**2\n",
    "print(y)\n",
    "v = torch.tensor([1, 1], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x = \n",
    " \\left|\n",
    " \\begin{matrix}\n",
    "   x_1 \\\\\n",
    "   x_2\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "  \\left|\n",
    " \\begin{matrix}\n",
    "   2 \\\\\n",
    "   3\n",
    " \\end{matrix}\n",
    " \\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \n",
    " \\left|\n",
    " \\begin{matrix}\n",
    "   y_1 \\\\\n",
    "   y_2\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "  \\left|\n",
    " \\begin{matrix}\n",
    "   {x_1}^2 + 3\\times x_2 \\\\\n",
    "   2 \\times x_1 + {x_2}^2\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "   \\left|\n",
    " \\begin{matrix}\n",
    "   13 \\\\\n",
    "   13\n",
    " \\end{matrix}\n",
    " \\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{ \\partial x} = \n",
    " \\left|\n",
    " \\begin{matrix}\n",
    "   \\frac{\\partial y1}{ \\partial x1} & \\frac{\\partial y1}{ \\partial x2} \\\\\n",
    "   \\frac{\\partial y2}{ \\partial x1} & \\frac{\\partial y2}{ \\partial x2}\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "  \\left|\n",
    " \\begin{matrix}\n",
    "   2x_1 & 3\\\\\n",
    "   2  & 2x_2\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "   \\left|\n",
    " \\begin{matrix}\n",
    "   4 & 3\\\\\n",
    "   2  & 6\n",
    " \\end{matrix}\n",
    " \\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "v = \n",
    " \\left|\n",
    " \\begin{matrix}\n",
    "   v_1 \\\\\n",
    "   v_2\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "  \\left|\n",
    " \\begin{matrix}\n",
    "   1 \\\\\n",
    "   1\n",
    " \\end{matrix}\n",
    " \\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "v \\times \\frac{\\partial y}{ \\partial x}\n",
    "=\n",
    "  \\left|\n",
    " \\begin{matrix}\n",
    "   1 &   1\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " \\times\n",
    "  \\left|\n",
    " \\begin{matrix}\n",
    "   4 & 3\\\\\n",
    "   2  & 6\n",
    " \\end{matrix}\n",
    " \\right|\n",
    " =\n",
    "   \\left|\n",
    " \\begin{matrix}\n",
    "   6 & 9\n",
    " \\end{matrix}\n",
    " \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 要求x的梯度的shape和x一致, 给了不同维度的v，也没有用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([13., 13.], grad_fn=<CopySlices>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([12., 18.])\n",
      "tensor([24., 36.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2, 3], dtype=torch.float, requires_grad=True)\n",
    "print(x)\n",
    "y = torch.zeros(2)\n",
    "y[0] = x[0]**2 + x[1] * 3\n",
    "y[1] = x[0] * 2 + x[1]**2\n",
    "print(y)\n",
    "v1 = torch.tensor([[1, 1], [1, 1]], dtype=torch.float)\n",
    "print(v1)\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(x.grad)\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何计算梯度呢？  (这里给向量的例子，其他维度的...）\n",
    "retain_graph=True 会把雅可比矩阵和梯度保存, 但是每次手动把梯度置0,不然梯度会加上去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True)\n",
      "tensor([13., 13.], grad_fn=<CopySlices>)\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2, 3], dtype=torch.float, requires_grad=True)\n",
    "print(x)\n",
    "y = torch.zeros(2)\n",
    "y[0] = x[0]**2 + x[1] * 3\n",
    "y[1] = x[0] * 2 + x[1]**2\n",
    "print(y)\n",
    "\n",
    "row, column = y.shape[0], x.shape[0]\n",
    "print(row, column)\n",
    "jacobian = torch.zeros(row, column, dtype=torch.float)\n",
    "zz = torch.zeros(column, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 3.])\n",
      "tensor([2., 6.])\n",
      "tensor([[4., 3.],\n",
      "        [2., 6.]])\n"
     ]
    }
   ],
   "source": [
    "for r in range(row):\n",
    "    temp_v = zz.clone()\n",
    "    temp_v[r] = 1\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()  # 这一步很重要！！！！！！！\n",
    "    y.backward(temp_v, retain_graph=True)\n",
    "    print(x.grad)\n",
    "    jacobian[r, :] = x.grad\n",
    "\n",
    "print(jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
